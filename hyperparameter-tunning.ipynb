{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Goal: Compare four Python-based HPO libraries:\n\n1. Optuna -  Bayesian optimization\n\n2. Hyperopt - Bayesian optimization with TPE\n\n3. Optunity - Evolutionary/nature-inspired algorithms\n\n4. SMAC (Sequential Model-Based Algorithm Configuration)\n\n### Benchmarking Criteria:\n\n1. CASH Problem (Combined Algorithm Selection and Hyperparameter optimization):\n\n2. NeurIPS Black-box Optimization Challenge:\n","metadata":{}},{"cell_type":"code","source":"pip install openml","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T17:30:57.661959Z","iopub.execute_input":"2025-04-19T17:30:57.662456Z","iopub.status.idle":"2025-04-19T17:31:01.608775Z","shell.execute_reply.started":"2025-04-19T17:30:57.662424Z","shell.execute_reply":"2025-04-19T17:31:01.607483Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import openml\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import f1_score\nfrom sklearn.naive_bayes import BernoulliNB, MultinomialNB\nfrom sklearn.ensemble import ExtraTreesClassifier, GradientBoostingClassifier, RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression, SGDClassifier\nfrom sklearn.svm import SVC\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nimport optuna\nimport time\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nfrom hyperopt import fmin, tpe, hp, Trials","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T18:31:37.677529Z","iopub.execute_input":"2025-04-19T18:31:37.678387Z","iopub.status.idle":"2025-04-19T18:31:37.687737Z","shell.execute_reply.started":"2025-04-19T18:31:37.678358Z","shell.execute_reply":"2025-04-19T18:31:37.686661Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataset_ids = {\n    \"dna\": 40670,\n    \"electricity\": 151,    \n    \"gas_drift\": 1476,     \n    \"nomao\": 1486,\n    \"pendigits\": 32,\n    \"semeion\": 1501,      \n}\ndatasets = {}\n\nfor name, did in dataset_ids.items():\n    print(f\"Downloading: {name}\")\n    d = openml.datasets.get_dataset(did)\n    X, y, _, _ = d.get_data(target=d.default_target_attribute)\n    df = pd.concat([X, y], axis=1)\n    datasets[name] = df\n    print(f\"{name} â†’ shape: {df.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T17:31:01.619308Z","iopub.execute_input":"2025-04-19T17:31:01.619685Z","iopub.status.idle":"2025-04-19T17:31:02.011058Z","shell.execute_reply.started":"2025-04-19T17:31:01.619653Z","shell.execute_reply":"2025-04-19T17:31:02.010181Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def info(m):\n    print(datasets[m].head())\n    print(datasets[m].info())\n    col_name = [col for col in datasets[m].columns if col.lower() == \"class\"][0]\n    print('Total Class:', datasets[m][col_name].unique())\n\ninfo('electricity')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T17:31:02.011945Z","iopub.execute_input":"2025-04-19T17:31:02.012256Z","iopub.status.idle":"2025-04-19T17:31:02.037509Z","shell.execute_reply.started":"2025-04-19T17:31:02.012234Z","shell.execute_reply":"2025-04-19T17:31:02.036080Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### K fold cross validation","metadata":{}},{"cell_type":"code","source":"# where K = 3\ndef kfold_cross_validation(model, X, y):\n    kf = KFold(n_splits=3, shuffle=True, random_state=42)\n    f1_scores = []\n\n    for train_index, val_index in kf.split(X):\n        X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n        y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n\n        model.fit(X_train, y_train)\n        preds = model.predict(X_val)\n        score = f1_score(y_val, preds, average='weighted')\n        f1_scores.append(score)\n\n    return np.mean(f1_scores)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T17:31:02.039199Z","iopub.execute_input":"2025-04-19T17:31:02.039609Z","iopub.status.idle":"2025-04-19T17:31:02.158340Z","shell.execute_reply.started":"2025-04-19T17:31:02.039558Z","shell.execute_reply":"2025-04-19T17:31:02.156075Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Benchmark 1\n\n1. Bernoulli naive Bayes \n2. Multinomial naive Bayes \n3. Decision Tree \n4. Extra Trees \n5. Gradient Boosting \n6. Random Forest \n7. K Nearest Neighbors \n8. Logistic Regression \n9. Linear SVM \n10. SGD Classifier \n11. XGB Classifier \n12. LGBM Classifier \n\nA total of 58 hyperparameters, including continuous and categorical","metadata":{}},{"cell_type":"markdown","source":"### Optuna","metadata":{}},{"cell_type":"code","source":"def objective(trial, X, y, classifier):\n    if classifier == 'bernoulliNB':\n        binarize = trial.suggest_categorical('binarize', [0.0, 0.5, 1.0])\n        alpha = trial.suggest_float('alpha', 0.0001, 1.0)\n        model = BernoulliNB(binarize=binarize, alpha=alpha)\n        return kfold_cross_validation(model, X, y)\n        \n    elif classifier == 'multinomialNB':\n        alpha = trial.suggest_float('alpha', 0.0001, 1.0)\n        fit_prior = trial.suggest_categorical('fit_prior', [True, False])\n        model = MultinomialNB(alpha, fit_prior)\n        return kfold_cross_validation(model, X, y)\n\n    elif classifier == 'DecisionTree':\n        criterion = trial.suggest_categorical(\"criterion\", ['gini','entropy'])\n        max_depth = trial.suggest_int('max_depth',2,32)\n        min_samples_split=trial.suggest_int(\"min_samples_split\", 2, 10)\n        max_features = trial.suggest_categorical('max_features', ['sqrt', 'log2', None])\n        model = DecisionTreeClassifier(criterion=criterion,max_depth=max_depth,\n                                       min_samples_split=min_samples_split,\n                                       max_features=max_features,random_state=42)\n        return kfold_cross_validation(model, X, y)\n\n    elif classifier == 'ExtraTrees':\n        n_estimators=trial.suggest_int(\"n_estimators\", 50, 200),\n        criterion=trial.suggest_categorical(\"criterion\", [\"gini\", \"entropy\"]),\n        max_depth=trial.suggest_int(\"max_depth\", 2, 32),\n        min_samples_split=trial.suggest_int(\"min_samples_split\", 2, 10),\n        max_features=trial.suggest_categorical(\"max_features\", [\"sqrt\", \"log2\", None])\n        model = ExtraTreesClassifier(n_estimators=n_estimators,max_depth=max_depth,max_features=max_features,\n                                     min_samples_split=min_samples_split,min_samples_leaf=min_samples_leaf,\n                                     random_state=42,n_jobs=-1)\n        return kfold_cross_validation(model, X, y)\n\n    elif classifier == 'GradientBoosting':\n        n_estimators=trial.suggest_int(\"n_estimators\", 50, 200),\n        learning_rate=trial.suggest_float(\"learning_rate\", 0.01, 0.3),\n        subsample=trial.suggest_float(\"subsample\", 0.5, 1.0),\n        max_depth=trial.suggest_int(\"max_depth\", 2, 32),\n        min_samples_split=trial.suggest_int(\"min_samples_split\", 2, 10)\n        model = GradientBoostingClassifier(n_estimators=n_estimators,learning_rate=learning_rate,max_depth=max_depth,\n                                           min_samples_split=min_samples_split,min_samples_leaf=min_samples_leaf,\n                                           max_features=max_features,random_state=42)\n        return kfold_cross_validation(model, X, y)\n\n    elif classifier == 'RandomForest':\n        model = RandomForestClassifier(\n            n_estimators=trial.suggest_int(\"n_estimators\", 50, 200),\n            max_depth=trial.suggest_int(\"max_depth\", 2, 32),\n            min_samples_split=trial.suggest_int(\"min_samples_split\", 2, 10),\n            max_features=trial.suggest_categorical(\"max_features\", [\"sqrt\", \"log2\", None]),\n            random_state=42\n        )\n        return kfold_cross_validation(model, X, y)\n\n    elif classifier == 'KNN':\n        model = KNeighborsClassifier(\n            n_neighbors=trial.suggest_int(\"n_neighbors\", 3, 15),\n            weights=trial.suggest_categorical(\"weights\", [\"uniform\", \"distance\"]),\n            algorithm=trial.suggest_categorical(\"algorithm\", [\"auto\", \"ball_tree\", \"kd_tree\", \"brute\"])\n        )\n        return kfold_cross_validation(model, X, y)\n    \n    elif classifier == 'LogisticRegression':\n        model = LogisticRegression(\n            penalty=trial.suggest_categorical(\"penalty\", [\"l2\", \"l1\"]),\n            C=trial.suggest_float(\"C\", 1e-3, 10.0),\n            solver=trial.suggest_categorical(\"solver\", [\"lbfgs\", \"saga\"]),\n            max_iter=trial.suggest_int(\"max_iter\", 100, 1000),\n            random_state=42\n        )\n        return kfold_cross_validation(model, X, y)\n    \n    elif classifier == 'SVC':\n        model = SVC(\n            C=trial.suggest_float(\"C\", 1e-3, 10.0),\n            kernel=trial.suggest_categorical(\"kernel\", [\"linear\", \"rbf\", \"poly\"]),\n            gamma=trial.suggest_categorical(\"gamma\", [\"scale\", \"auto\"]),\n            probability=True\n        )\n        return kfold_cross_validation(model, X, y)\n    \n    elif classifier == 'SGDClassifier':\n        model = SGDClassifier(\n            loss=trial.suggest_categorical(\"loss\", [\"hinge\", \"log_loss\", \"modified_huber\"]),\n            penalty=trial.suggest_categorical(\"penalty\", [\"l2\", \"l1\", \"elasticnet\"]),\n            alpha=trial.suggest_float(\"alpha\", 1e-5, 1e-1),\n            learning_rate=trial.suggest_categorical(\"learning_rate\", [\"constant\", \"optimal\", \"invscaling\"]),\n            eta0=trial.suggest_float(\"eta0\", 1e-3, 1.0),\n            random_state=42\n        )\n        return kfold_cross_validation(model, X, y)\n    \n    elif classifier == 'XGBClassifier':\n        model = XGBClassifier(\n            n_estimators=trial.suggest_int(\"n_estimators\", 50, 200),\n            max_depth=trial.suggest_int(\"max_depth\", 3, 10),\n            learning_rate=trial.suggest_float(\"learning_rate\", 0.01, 0.3),\n            subsample=trial.suggest_float(\"subsample\", 0.5, 1.0),\n            colsample_bytree=trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n            use_label_encoder=False,\n            eval_metric='logloss',\n            random_state=42\n        )\n        return kfold_cross_validation(model, X, y)\n    \n    elif classifier == 'LGBMClassifier':\n        model = LGBMClassifier(\n            n_estimators=trial.suggest_int(\"n_estimators\", 50, 200),\n            learning_rate=trial.suggest_float(\"learning_rate\", 0.01, 0.3),\n            max_depth=trial.suggest_int(\"max_depth\", 3, 10),\n            num_leaves=trial.suggest_int(\"num_leaves\", 20, 100),\n            subsample=trial.suggest_float(\"subsample\", 0.5, 1.0),\n            colsample_bytree=trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n            random_state=42\n        )\n        return kfold_cross_validation(model, X, y)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T17:46:27.156269Z","iopub.execute_input":"2025-04-19T17:46:27.157281Z","iopub.status.idle":"2025-04-19T17:46:27.180056Z","shell.execute_reply.started":"2025-04-19T17:46:27.157250Z","shell.execute_reply":"2025-04-19T17:46:27.178922Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import time\n\ndef run_optuna_search_TPE(X, y, classifier):\n    start_time = time.time()\n    study = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler())\n    study.optimize(lambda trial: objective(trial,X,y, classifier), n_trials=50)\n\n    end_time =time.time()\n\n    best_score = study.best_value\n    best_params = study.best_params\n    time_taken = end_time - start_time\n\n    return best_score, time_taken, best_params","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T18:19:06.462985Z","iopub.execute_input":"2025-04-19T18:19:06.463337Z","iopub.status.idle":"2025-04-19T18:19:06.471310Z","shell.execute_reply.started":"2025-04-19T18:19:06.463311Z","shell.execute_reply":"2025-04-19T18:19:06.470034Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"models = ['BernoulliNB', 'MultinomialNB', 'DecisionTree', 'ExtraTrees', 'GradientBoosting',\n          'RandomForest', 'KNN', 'LogisticRegression', 'SVC', 'SGDClassifier', 'XGBClassifier', 'LGBMClassifier']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T18:12:30.638544Z","iopub.execute_input":"2025-04-19T18:12:30.639391Z","iopub.status.idle":"2025-04-19T18:12:30.644061Z","shell.execute_reply.started":"2025-04-19T18:12:30.639361Z","shell.execute_reply":"2025-04-19T18:12:30.643165Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X = datasets['dna'].iloc[:,:-1]\ny = datasets['dna'].iloc[:,-1]\n\nscore, time, params = run_optuna_search_TPE(X, y, 'DecisionTree')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T18:19:09.549565Z","iopub.execute_input":"2025-04-19T18:19:09.549912Z","iopub.status.idle":"2025-04-19T18:19:29.659063Z","shell.execute_reply.started":"2025-04-19T18:19:09.549889Z","shell.execute_reply":"2025-04-19T18:19:29.658262Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(score)\nprint(params)\nprint(time)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T18:20:05.753412Z","iopub.execute_input":"2025-04-19T18:20:05.753750Z","iopub.status.idle":"2025-04-19T18:20:05.759152Z","shell.execute_reply.started":"2025-04-19T18:20:05.753725Z","shell.execute_reply":"2025-04-19T18:20:05.758128Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import time\ndef run_optuna_search_random(X, y, classifier):\n    start_time = time.time()\n    study = optuna.create_study(direction='maximize')\n    study.optimize(lambda trial: objective(trial,X,y, classifier), n_trials=50)\n\n    end_time =time.time()\n\n    best_score = study.best_value\n    best_params = study.best_params\n    time_taken = end_time - start_time\n\n    return best_score, time_taken, best_params\n\nscore, time, params = run_optuna_search_ramdom(X, y, 'DecisionTree')\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T18:17:55.820243Z","iopub.execute_input":"2025-04-19T18:17:55.820596Z","iopub.status.idle":"2025-04-19T18:18:15.931332Z","shell.execute_reply.started":"2025-04-19T18:17:55.820554Z","shell.execute_reply":"2025-04-19T18:18:15.930370Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(score)\nprint(params)\nprint(time)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T18:18:27.660928Z","iopub.execute_input":"2025-04-19T18:18:27.661229Z","iopub.status.idle":"2025-04-19T18:18:27.666273Z","shell.execute_reply.started":"2025-04-19T18:18:27.661208Z","shell.execute_reply":"2025-04-19T18:18:27.665301Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### HyperOPT","metadata":{}},{"cell_type":"code","source":"def objective(params, classifier, X, y):\n    if classifier == 'bernoulliNB':\n        binarize = params['binarize']\n        alpha = params['alpha']\n        model = BernoulliNB(binarize=binarize, alpha=alpha)\n        return kfold_cross_validation(model, X, y)\n\n    elif classifier == 'multinomialNB':\n        alpha = params['alpha']\n        fit_prior = params['fit_prior']\n        model = MultinomialNB(alpha=alpha, fit_prior=fit_prior)\n        return kfold_cross_validation(model, X, y)\n\n    elif classifier == 'DecisionTree':\n        criterion = params['criterion']\n        max_depth = params['max_depth']\n        min_samples_split = params['min_samples_split']\n        max_features = params['max_features']\n        model = DecisionTreeClassifier(criterion=criterion, max_depth=max_depth,\n                                       min_samples_split=min_samples_split,\n                                       max_features=max_features, random_state=42)\n        return kfold_cross_validation(model, X, y)\n\n    elif classifier == 'ExtraTrees':\n        n_estimators = params['n_estimators']\n        criterion = params['criterion']\n        max_depth = params['max_depth']\n        min_samples_split = params['min_samples_split']\n        max_features = params['max_features']\n        model = ExtraTreesClassifier(n_estimators=n_estimators, max_depth=max_depth,\n                                     max_features=max_features, min_samples_split=min_samples_split,\n                                     random_state=42)\n        return kfold_cross_validation(model, X, y)\n\n    elif classifier == 'GradientBoosting':\n        n_estimators = params['n_estimators']\n        learning_rate = params['learning_rate']\n        subsample = params['subsample']\n        max_depth = params['max_depth']\n        min_samples_split = params['min_samples_split']\n        model = GradientBoostingClassifier(n_estimators=n_estimators, learning_rate=learning_rate,\n                                           max_depth=max_depth, min_samples_split=min_samples_split,\n                                           subsample=subsample, random_state=42)\n        return kfold_cross_validation(model, X, y)\n\n    elif classifier == 'RandomForest':\n        model = RandomForestClassifier(\n            n_estimators=params['n_estimators'],\n            max_depth=params['max_depth'],\n            min_samples_split=params['min_samples_split'],\n            max_features=params['max_features'],\n            random_state=42\n        )\n        return kfold_cross_validation(model, X, y)\n\n    elif classifier == 'KNN':\n        model = KNeighborsClassifier(\n            n_neighbors=params['n_neighbors'],\n            weights=params['weights'],\n            algorithm=params['algorithm']\n        )\n        return kfold_cross_validation(model, X, y)\n    \n    elif classifier == 'LogisticRegression':\n        model = LogisticRegression(\n            penalty=params['penalty'],\n            C=params['C'],\n            solver=params['solver'],\n            max_iter=params['max_iter'],\n            random_state=42\n        )\n        return kfold_cross_validation(model, X, y)\n    \n    elif classifier == 'SVC':\n        model = SVC(\n            C=params['C'],\n            kernel=params['kernel'],\n            gamma=params['gamma'],\n            probability=True\n        )\n        return kfold_cross_validation(model, X, y)\n    \n    elif classifier == 'SGDClassifier':\n        model = SGDClassifier(\n            loss=params['loss'],\n            penalty=params['penalty'],\n            alpha=params['alpha'],\n            learning_rate=params['learning_rate'],\n            eta0=params['eta0'],\n            random_state=42\n        )\n        return kfold_cross_validation(model, X, y)\n    \n    elif classifier == 'XGBClassifier':\n        model = XGBClassifier(\n            n_estimators=params['n_estimators'],\n            max_depth=params['max_depth'],\n            learning_rate=params['learning_rate'],\n            subsample=params['subsample'],\n            colsample_bytree=params['colsample_bytree'],\n            use_label_encoder=False,\n            eval_metric='logloss',\n            random_state=42\n        )\n        return kfold_cross_validation(model, X, y)\n    \n    elif classifier == 'LGBMClassifier':\n        model = LGBMClassifier(\n            n_estimators=params['n_estimators'],\n            learning_rate=params['learning_rate'],\n            max_depth=params['max_depth'],\n            num_leaves=params['num_leaves'],\n            subsample=params['subsample'],\n            colsample_bytree=params['colsample_bytree'],\n            random_state=42\n        )\n        return kfold_cross_validation(model, X, y)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T18:37:21.910633Z","iopub.execute_input":"2025-04-19T18:37:21.911547Z","iopub.status.idle":"2025-04-19T18:37:21.925532Z","shell.execute_reply.started":"2025-04-19T18:37:21.911513Z","shell.execute_reply":"2025-04-19T18:37:21.924473Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_search_space(classifier):\n    if classifier == 'bernoulliNB':\n        return {\n            'binarize': hp.choice('binarize', [0.0, 0.5, 1.0]),\n            'alpha': hp.float('alpha', 0.0001, 1.0)\n        }\n\n    elif classifier == 'multinomialNB':\n        return {\n            'alpha': hp.float('alpha', 0.0001, 1.0),\n            'fit_prior': hp.choice('fit_prior', [True, False])\n        }\n\n    elif classifier == 'DecisionTree':\n        return {\n            'criterion': hp.choice('criterion', ['gini', 'entropy']),\n            'max_depth': hp.randint('max_depth', 2, 32),\n            'min_samples_split': hp.randint('min_samples_split', 2, 10),\n            'max_features': hp.choice('max_features', ['sqrt', 'log2', None])\n        }\n\n    elif classifier == 'ExtraTrees':\n        return {\n            'n_estimators': hp.int('n_estimators', 50, 200),\n            'criterion': hp.choice('criterion', ['gini', 'entropy']),\n            'max_depth': hp.randint('max_depth', 2, 32),\n            'min_samples_split': hp.randint('min_samples_split', 2, 10),\n            'max_features': hp.choice('max_features', ['sqrt', 'log2', None])\n        }\n\n    elif classifier == 'GradientBoosting':\n        return {\n            'n_estimators': hp.randint('n_estimators', 50, 200),\n            'learning_rate': hp.float('learning_rate', 0.01, 0.3),\n            'subsample': hp.float('subsample', 0.5, 1.0),\n            'max_depth': hp.randint('max_depth', 2, 32),\n            'min_samples_split': hp.randint('min_samples_split', 2, 10)\n        }\n\n    elif classifier == 'RandomForest':\n        return {\n            'n_estimators': hp.randint('n_estimators', 50, 200),\n            'max_depth': hp.randint('max_depth', 2, 32),\n            'min_samples_split': hp.randint('min_samples_split', 2, 10),\n            'max_features': hp.choice('max_features', ['sqrt', 'log2', None])\n        }\n\n    elif classifier == 'KNN':\n        return {\n            'n_neighbors': hp.randint('n_neighbors', 3, 15),\n            'weights': hp.choice('weights', ['uniform', 'distance']),\n            'algorithm': hp.choice('algorithm', ['auto', 'ball_tree', 'kd_tree', 'brute'])\n        }\n\n    elif classifier == 'LogisticRegression':\n        return {\n            'penalty': hp.choice('penalty', ['l2', 'l1']),\n            'C': hp.float('C', 1e-3, 10.0),\n            'solver': hp.choice('solver', ['lbfgs', 'saga']),\n            'max_iter': hp.randint('max_iter', 100, 1000)\n        }\n\n    elif classifier == 'SVC':\n        return {\n            'C': hp.float('C', 1e-3, 10.0),\n            'kernel': hp.choice('kernel', ['linear', 'rbf', 'poly']),\n            'gamma': hp.choice('gamma', ['scale', 'auto'])\n        }\n\n    elif classifier == 'SGDClassifier':\n        return {\n            'loss': hp.choice('loss', ['hinge', 'log_loss', 'modified_huber']),\n            'penalty': hp.choice('penalty', ['l2', 'l1']),\n            'alpha': hp.uniform('alpha', 1e-5, 1e-1),\n            'learning_rate': hp.choice('learning_rate', ['constant', 'optimal', 'invscaling']),\n            'eta0': hp.uniform('eta0', 1e-3, 1.0)\n        }\n    elif classifier == 'XGBClassifier':\n        return {\n            'n_estimators': hp.randint('n_estimators', 50, 200),\n            'max_depth': hp.randint('max_depth', 3, 10),\n            'learning_rate': hp.uniform('learning_rate', 0.01, 0.3),\n            'subsample': hp.uniform('subsample', 0.5, 1.0),\n            'colsample_bytree': hp.uniform('colsample_bytree', 0.5, 1.0)\n        }\n\n    elif classifier == 'LGBMClassifier':\n        return {\n            'n_estimators': hp.randint('n_estimators', 50, 200),\n            'learning_rate': hp.uniform('learning_rate', 0.01, 0.3),\n            'max_depth': hp.randint('max_depth', 3, 10),\n            'num_leaves': hp.randint('num_leaves', 20, 100),\n            'subsample': hp.uniform('subsample', 0.5, 1.0),\n            'colsample_bytree': hp.uniform('colsample_bytree', 0.5, 1.0)\n        }                     ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T18:55:12.041768Z","iopub.execute_input":"2025-04-19T18:55:12.042125Z","iopub.status.idle":"2025-04-19T18:55:12.057508Z","shell.execute_reply.started":"2025-04-19T18:55:12.042103Z","shell.execute_reply":"2025-04-19T18:55:12.056528Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import time\n\ndef hyperOPT_TPE(classifier, X,y):\n    start_time = time.time()\n    trials = Trials()\n\n    best = fmin(\n        fn=lambda params: objective(params, X, y, classifier),\n        space=get_search_space(classifier),\n        algo=tpe.suggest,\n        max_evals=50,  \n        trials=trials\n    )\n    end_time = time.time()\n    total_time = end_time - start_time\n    return total_time, best","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T18:55:15.399485Z","iopub.execute_input":"2025-04-19T18:55:15.399826Z","iopub.status.idle":"2025-04-19T18:55:15.405873Z","shell.execute_reply.started":"2025-04-19T18:55:15.399800Z","shell.execute_reply":"2025-04-19T18:55:15.404971Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"time, best = hyperOPT_TPE('RandomForest',X,y)\nprint(time)\nprint(best)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T19:05:28.103956Z","iopub.execute_input":"2025-04-19T19:05:28.104307Z","iopub.status.idle":"2025-04-19T19:06:06.386739Z","shell.execute_reply.started":"2025-04-19T19:05:28.104286Z","shell.execute_reply":"2025-04-19T19:06:06.385918Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import time\n\ndef hyperOPT_Rand(classifier, X,y):\n    start_time = time.time()\n    trials = Trials()\n\n    best = fmin(\n        fn=lambda params: objective(params, X, y, classifier),\n        space=get_search_space(classifier),\n        algo=rand.suggest,\n        max_evals=50,  \n        trials=trials\n    )\n    end_time = time.time()\n    total_time = end_time - start_time\n    return total_time, best","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T19:09:18.279334Z","iopub.execute_input":"2025-04-19T19:09:18.280507Z","iopub.status.idle":"2025-04-19T19:09:18.286779Z","shell.execute_reply.started":"2025-04-19T19:09:18.280462Z","shell.execute_reply":"2025-04-19T19:09:18.285625Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"time, best = hyperOPT_TPE('RandomForest',X,y)\nprint(time)\nprint(best)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T19:09:24.585267Z","iopub.execute_input":"2025-04-19T19:09:24.585613Z","iopub.status.idle":"2025-04-19T19:10:01.830477Z","shell.execute_reply.started":"2025-04-19T19:09:24.585567Z","shell.execute_reply":"2025-04-19T19:10:01.829387Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Optunity","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}